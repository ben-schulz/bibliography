# text cleaning

[Craig Trim. "The Art of Tokenization"](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)

# hyperspace analogue to language (hal)

[Lund and Burgess. "Producing high-dimensional semantic spaces from lexical co-occurrence"](https://link.springer.com/content/pdf/10.3758%2FBF03204766.pdf)

[Hinrich Schutze. "Automatic Word Sense Discrimination"](https://www.aclweb.org/anthology/J98-1004)
