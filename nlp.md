# text cleaning

[Craig Trim. "The Art of Tokenization"](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)

# hyperspace analogue to language (hal)

[Lund and Burgess. "Producing high-dimensional semantic spaces from lexical co-occurrence"](https://link.springer.com/content/pdf/10.3758%2FBF03204766.pdf)

[Hinrich Schutze. "Automatic Word Sense Discrimination"](http://delivery.acm.org/10.1145/980000/972724/p97-schutze.pdf?ip=47.233.78.79&id=972724&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1563727497_0dbfd81abdfcab77a75b5f891cf72092)
